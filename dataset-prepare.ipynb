{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ce04e",
   "metadata": {},
   "source": [
    "因为map dataset需要时间比较久（~15min），因此单独进行，然后保存下来，训练的时候只需要直接load 已经map好的dataset就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "found-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset,load_dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f06ac",
   "metadata": {},
   "source": [
    "这里和原代码稍有不同，因为直接使用load_dataset方法load txt文件在我的环境里会有问题，不报错也不运行。因此改成了通过pandas中转一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c8616",
   "metadata": {},
   "source": [
    "根据原代码里给出的wiki句子数据集的下载路径下好数据集（ https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt ），然后 wiki_text_file = 你的数据集路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-tradition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 995447\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to read simCSE-wiki.txt\n",
    "wiki_text_file = 'your-path-to/simCSE-wiki.txt'\n",
    "wiki = pd.read_csv(wiki_text_file,sep = '\\t',header = None)\n",
    "wiki.columns = ['text']\n",
    "# use Dataset.from_pandas to convert to dataset\n",
    "wiki_dataset = Dataset.from_pandas(wiki,split= \"train\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threatened-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    \n",
    "    total = len(examples['text'])\n",
    "    # total = batch_size\n",
    "    \n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "\n",
    "    sentences = examples['text'] + examples['text']\n",
    "\n",
    "    # set max_length here:\n",
    "    sent_features = tokenizer(sentences, max_length=32, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    features = {}\n",
    "    for key in sent_features:\n",
    "        features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-capability",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a298b723ec4856822a05f7a4210123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/249 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = wiki_dataset.map(prepare_features,batched=True, remove_columns=['text'], batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fe35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hispanic-jersey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
       "    num_rows: 995447\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rotary-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk for reuse\n",
    "train_dataset.save_to_disk(\"wiki_for_sts_32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-choir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
